divolte {
  global {
    server {
      // The host to which the server binds.
      // Set to a specific IP address to selectively listen on that interface.
      host = 0.0.0.0
      host = ${?DIVOLTE_HOST}

      // The port on which the sever listens.
      port = 8290
      port = ${?DIVOLTE_PORT}

      // Whether to use the X-Forwarded-For header HTTP header
      // for determining the source IP of a request if present.
      // When a X-Forwared-For header is present, the rightmost
      // IP address of the value is used as source IP when
      // when multiple IP addresses are separated by a comma.
      // When the header is present more than once, the last
      // value will be used.
      // E.g.
      // "X-Forwarded-For: 10.200.13.28, 11.45.82.30" ==> 11.45.82.30
      //
      // "X-Forwarded-For: 10.200.13.28"
      // "X-Forwarded-For: 11.45.82.30" ==> 11.45.82.30
      use_x_forwarded_for = false
      use_x_forwarded_for = ${?DIVOLTE_USE_XFORWARDED_FOR}

      // When true Divolte Collector serves a static test page at /.
      serve_static_resources = true
      serve_static_resources = ${?DIVOLTE_SERVICE_STATIC_RESOURCES}
    }

    mapper {
      // Number of threads to use for processing incoming requests
      threads = 2
      threads = ${?DIVOLTE_INCOMING_REQUEST_PROCESSOR_THREADS}

      // This configures the ip2geo database for geo lookups. A ip2geo database
      // can be obtained from MaxMind (https://www.maxmind.com/en/geoip2-databases).
      // Both a free version and a more accurate paid version are available.
      //
      // By default, no ip2geo database is configured. When this setting is
      // absent, no attempt will be made to lookup geo-coordinates for IP
      // addresses. If configured, Divolte Collector will keep a filesystem
      // watch on the database file. If the file is changed on the filesystem
      // the database will be reloaded at runtime without requireing a restart.
      // ip2geo_database = /path/to/dabase/file.db

      user_agent_parser {
        // The parser type.
        type = non_updating
        type = ${?DIVOLTE_UA_PARSER_TYPE}

        // User agent parsing is a relatively expensive operation that requires
        // many regular expression evaluations. Very often the same user agent
        // will make consecutive requests and many clients will have the exact
        // same user agent as well. It therefore makes sense to cache the
        // parsing results in memory and do a lookup before trying a parse.
        // This setting determines how many unique user agent strings will be
        // cached.
        cache_size = 1000
        cache_size = ${?DIVOLTE_UA_PARSER_CACHE_SIZE}
      }

      // Browsers and other clients (e.g. anti-virus software, proxies)
      // will sometimes send the exact same request twice. Divolte
      // Collector attempts to flag these duplicate events, by using
      // a internal probabilistic data structure with a finite memory
      // size. The memory consists internally of an array of 64 bit
      // integers. This the memory required in bytes is the memory size
      // times 8 (8 megabytes for 1 million entries).
      // Note that the memory size is per thread.
      duplicate_memory_size = 1000000
      duplicate_memory_size = ${?DIVOLTE_INCOMING_REQUEST_PROCESSOR_DUPLICATE_MEMORY_SIZE}
    }

    kafka {
      // If true, flushing to Kafka is enabled.
      enabled = false
      enabled = ${?DIVOLTE_KAFKA_ENABLED}

      // Number of threads to use for flushing events to Kafka
      threads = 2
      threads = ${?DIVOLTE_KAFKA_THREADS}

      // The maximum queue of incoming requests to keep
      // before starting to drop incoming requests. Note
      // that when this queue is full, requests are dropped
      // and a warning is logged. No errors are reported to
      // the client side. Divolte Collector will always respond
      // with a HTTP 200 status code and the image payload.
      // Note that the queue size is per thread.
      buffer_size = 1048576
      buffer_size = ${?DIVOLTE_KAFKA_MAX_WRITE_QUEUE}

      // All settings in here are used as-is to configure
      // the Kafka producer.
      // See: http://kafka.apache.org/documentation.html#producerconfigs
      producer = {
        bootstrap.servers = ["localhost:9092"]
        bootstrap.servers = ${?DIVOLTE_KAFKA_BROKER_LIST}

        client.id = divolte.collector
        client.id = ${?DIVOLTE_KAFKA_CLIENT_ID}

        acks = 0
        acks = ${?DIVOLTE_KAFKA_REQUIRED_ACKS}
        retries = 5
        retries = ${?DIVOLTE_KAFKA_MAX_RETRIES}
      }
    }

    hdfs {
      // If true, flushing to HDFS is enabled.
      enabled = true
      enabled = ${?DIVOLTE_HDFS_ENABLED}

      // Number of threads to use for flushing events to HDFS.
      // Each thread creates its own files on HDFS. Depending
      // on the flushing strategy, multiple concurrent files
      // could be kept open per thread.
      threads = 2
      threads = ${?DIVOLTE_HDFS_THREADS}

      client {
        // default nonexistant: Use HADOOP_CONF_DIR on the classpath.
        // If not present empty config results in local filesystem being used.
        fs.defaultFS = "file:///"
        fs.defaultFS = ${?DIVOLTE_HDFS_URI}
      }
		}
  }

  sources {
    browser = {
      type = browser

      // The name of the cookie used for setting a party ID
      party_cookie = _dvp
      party_cookie = ${?DIVOLTE_PARTY_COOKIE}

      // The expiry time for the party ID cookie
      party_timeout = 730 days
      party_timeout = ${?DIVOLTE_PARTY_TIMEOUT}

      // The name of the cookie used tracking the session ID
      session_cookie = _dvs
      session_cookie = ${?DIVOLTE_SESSION_COOKIE}

      // The expiry time for a session
      session_timeout = 30 minutes
      session_timeout = ${?DIVOLTE_SESSION_TIMEOUT}

      // The cookie domain that is assigned to the cookies.
      // When left empty, the cookie will have no domain
      // explicitly associated with it, which effectively
      // sets it to the website domain of the page that
      // contains the Divolte Collector JavaScript.
      cookie_domain = ""
      cookie_domain = ${?DIVOLTE_COOKIE_DOMAIN}

      // The javascript section controls settings related to the way
      // the JavaScript file is compiled.
      javascript {
        // Name of the script file. This changes the divolte.js part in
        // the script url: http://www.domain.tld/divolte.js
        name = divolte.js
        name = ${?DIVOLTE_JAVASCRIPT_NAME}

        // Enable or disable the logging on the JavaScript console in
        // the browser
        logging = false
        logging = ${?DIVOLTE_JAVASCRIPT_LOGGING}

        // When true, the served JavaScript will be compiled, but not
        // minified, improving readability when debugging in the browser.
        debug = false
        debug = ${?DIVOLTE_JAVASCRIPT_DEBUG}

        // When false, divolte.js will not automatically send a pageView
        // event after being loaded. This way clients can send a initial
        // event themselves and have full control over the event type and
        // the custom parameters that are sent with the initial event.
        auto_page_view_event = true
        auto_page_view_event = ${?DIVOLTE_JAVASCRIPT_AUTO_PAGE_VIEW_EVENT}
      }
    }

    app = {
      type = json
    }
  }

  mappings {
    default = {
      // By default, Divolte Collector will use a built-in Avro schema for
      // writing data and a default mapping, which is documented in the
      // Mapping section of the user documentation. The default schema
      // can be found here: https://github.com/divolte/divolte-schema
      //
      // Typically, users will configure their own schema, usually with
      // fields specific to their domain and custom events and other
      // mappings. When using a user defined schema, it is also
      // required to provide a mapping script. See the user documentation
      // for further reference.
      schema_file = ${?DIVOLTE_SCHEMA_FILE}

      // The groovy script file to use as mapping definition.
      mapping_script_file = ${?DIVOLTE_SCHEMA_MAPPING_SCRIPT_FILE}

      // The sources to apply this mapping to
      sources = [browser,app]

      // The sinks to use
      sinks = [kafka,hdfs]

      // The incoming request handler attempts to parse out all
      // relevant information from the request as passed by the
      // JavaScript. If the incoming request appears corrupt,
      // for example because of a truncated URL or incorrect
      // data in the fields, the request is flagged as corrupt.
      // The detection of corrupt requests is enforced by appending
      // a hash of all fields to the request from the JavaScript.
      // This hash is validated on the server side.
      // If this setting is true, events that are flagged as corrupt
      // will be dropped from the stream, instead of processed further.
      // It is common not to drop the corrupt events, but instead
      // include them for later analysis.
      discard_corrupted = false
      discard_corrupted = ${?DIVOLTE_INCOMING_REQUEST_PROCESSOR_DISCARD_CORRUPTED}
    }
  }

  sinks {
    kafka = {
      type = kafka

      // The Kafka topic onto which events are published.
      topic = "divolte"

      // The topic can be overridden by setting the
      // DIVOLTE_KAFKA_TOPIC environment variable.
      topic = ${?DIVOLTE_KAFKA_TOPIC}
    }

    hdfs = {
      type = hdfs

      // Divolte Collector has two strategies for creating files
      // on HDFS and flushing data. By default, a simple rolling
      // file strategy is empoyed. This opens one file per thread
      // and rolls on to a new file after a configurable interval.
      // Files that are being written to, have a extension of
      // .avro.partial and are written the the directory configured
      // in the working_dir setting. When a file is closed, it
      // will be renamed to have a .avro extension and is moved to
      // the directory configured in the publish_dir settins. This
      // happens in a single (atomic) filesystem move operation.
      file_strategy {
        // Roll over files on HDFS after this amount of time.
        roll_every = 60 minutes
        roll_every = ${?DIVOLTE_HDFS_ROLL_EVERY}
        roll_every = 1 minutes

        // Issue a hsync against files each time this number of
        // records has been flushed to it.
        sync_file_after_records = 1000
        sync_file_after_records = ${?DIVOLTE_HDFS_SYNC_FILE_AFTER_RECORDS}
        sync_file_after_records = 10

        // If no records are being flushed, issue a hsync when
        // this amount of time passes, regardless of how much
        // data was written.
        sync_file_after_duration = 30 seconds
        sync_file_after_duration = ${?DIVOLTE_HDFS_SYNC_FILE_AFTER_DURATION}

        // Directory where files are created and kept while being
        // written to.
        // working_dir = /tmp
        // working_dir = ${?DIVOLTE_HDFS_WORKING_DIR}

        // Directory where files are moved to, after they are closed.
        // publish_dir = /tmp
        // publish_dir = ${?DIVOLTE_HDFS_PUBLISH_DIR}

        working_dir = /mnt/divolte_clicklogs/inflight
        publish_dir = /mnt/divolte_clicklogs/published
      }

      // The HDFS replication factor to use when creating
      // files.
      replication = 1
      replication = ${?DIVOLTE_HDFS_REPLICATION}
    }
  }
}